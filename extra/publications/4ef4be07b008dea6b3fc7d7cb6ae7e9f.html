<style>.gsc_oms_mm{font-size:24px;line-height:16px;display:inline-block;margin:-10px 0 0 4px;position:relative;top:8px;}.gsc_oms_link{white-space:nowrap;margin-right:12px;}.gsc_oms_link:last-child{margin-right:0;}#gsc_ocd_upload{max-width:500px;margin:0 auto;}.gsc_upl_title{font-size:20px;margin:0 0 4px 0;}.gsc_upl_desc{font-size:16px;padding:24px 0;}.gs_el_ph .gsc_upl_desc{padding:16px 0;}.gsc_upl_cprt{font-size:16px;color:#777;}#gsc_upl_error:empty,#gsc_upl_form{display:none;}#gsc_upl_error{padding:8px;margin-bottom:16px;}#gsc_vcd_title_wrapper{font-size:16px;margin:0 0 16px 0;}#gsc_vcd_title{font-size:20px;}.gs_el_sm #gsc_vcd_title{font-size:18px;}#gsc_vcd_title_gg{float:right;padding:0 0 8px 16px;}.gs_el_ph #gsc_vcd_title_gg{float:none;text-align:right;padding:0;margin:-4px 0 8px 0;}.gsc_vcd_title_ggt{font-size:13px;font-weight:bold;}.gsc_vcd_title_ggut{font-size:13px;color:#777;padding-bottom:8px;}.gsc_vcd_field{float:left;width:100px;text-align:right;color:#777;}.gs_el_ph .gsc_vcd_field{float:none;width:auto;text-align:left;margin-bottom:4px;font-size:16px;color:black;}.gsc_vcd_value{margin-left:116px;max-width:512px;margin-bottom:16px;}.gs_el_ph .gsc_vcd_value{margin-left:0;}.gsc_vcd_merged_snippet{margin-bottom:1em;}#gsc_vcd_graph{position:relative;height:100px;}#gsc_vcd_graph_x{position:absolute;top:57px;left:0;width:100%;height:13px;border-top:1px solid #777;}.gsc_vcd_g_t{position:absolute;top:60px;color:#777;font-size:11px;}#gsc_vcd_graph_bars{position:absolute;top:0;right:0;width:100%;height:100px;overflow-x:auto;}.gsc_vcd_g_a{position:absolute;bottom:13px;width:15px;background:#777;}.gsc_vcd_g_a:hover{background:#1a0dab;text-decoration:none;}.gsc_vcd_g_a:active{background:#d14836;}.gsc_vcd_g_al{position:absolute;bottom:15px;left:7px;color:#222;background:white;font-size:11px;min-width:11px;text-align:center;padding:1px;border:1px solid #777;border-radius:1px;visibility:hidden;opacity:0;}.gsc_vcd_g_a:hover .gsc_vcd_g_al{visibility:visible;opacity:1;}.gsc_vcd_g_a{transition:all .218s;}.gsc_vcd_g_al{transition:opacity .218s,visibility 0s .218s;}.gsc_vcd_g_a:hover,.gsc_vcd_g_a:hover .gsc_vcd_g_al{transition:all 0s;}a.gsc_vcd_lbx:link,a.gsc_vcd_lbx:visited{white-space:nowrap;display:inline-block;vertical-align:middle;font-size:11px;background-color:#e5e5e5;color:#777;padding:2px 6px;text-decoration:none;}a.gsc_vcd_lbx:hover{color:#e5e5e5;background-color:#777;}a.gsc_vcd_lbx:active{color:#d14836;}#gsc_ecd_alrt{padding:8px 16px;margin-bottom:12px;}#gsc_ecd_alrt:empty{display:none;}.gsc_ecd_field,.gsc_ecd_value{padding:6px 0;vertical-align:top;}.gsc_ecd_field{float:left;width:100px;text-align:right;color:#777;line-height:16px;padding:13px 0 6px 0;}.gs_el_ph .gsc_ecd_field,.gs_el_tc .gsc_ecd_field{float:none;width:auto;text-align:left;padding:8px 0 0 0;font-size:16px;color:black;}#gsc_ecd_citation_type{margin-bottom:12px;}.gsc_ecd_value,#gsc_ecd_citation_type{margin-left:116px;}.gs_el_ph .gsc_ecd_value,.gs_el_tc .gsc_ecd_value,.gs_el_ta #gsc_ecd_citation_type,.gs_el_ph #gsc_ecd_citation_type{margin-left:0;padding:8px 0;}.gs_el_ta #gsc_ecd_citation_type,.gs_el_ph #gsc_ecd_citation_type{text-align:center;}.gs_el_ph #gsc_ecd_citation_type{padding:0;margin-bottom:8px;}.gs_el_ph #gsc_ecd_title_value .gs_gray,.gs_el_tc #gsc_ecd_title_value .gs_gray,#gsc_ecd_reporter_value .gs_gray{display:none;}.gsc_ecd_reporter_line{margin-bottom:4px;}.gsc_ecd_merged_snippet{line-height:1.24;}.gsc_ecd_merged_radio{line-height:18px;margin:8px 0 16px 0;color:#555;}.gsc_ecd_type_journal,.gsc_ecd_type_conference,.gsc_ecd_type_chapter,.gsc_ecd_type_book,.gsc_ecd_type_thesis,.gsc_ecd_type_patent,.gsc_ecd_type_courtcase,.gsc_ecd_type_other{display:none;}.gsc_ecd_form_journal .gsc_ecd_type_journal,.gsc_ecd_form_conference .gsc_ecd_type_conference,.gsc_ecd_form_chapter .gsc_ecd_type_chapter,.gsc_ecd_form_book .gsc_ecd_type_book,.gsc_ecd_form_thesis .gsc_ecd_type_thesis,.gsc_ecd_form_patent .gsc_ecd_type_patent,.gsc_ecd_form_courtcase .gsc_ecd_type_courtcase,.gsc_ecd_form_other .gsc_ecd_type_other{display:block;}.gsc_ecd_form_journal span.gsc_ecd_type_journal,.gsc_ecd_form_conference span.gsc_ecd_type_conference,.gsc_ecd_form_chapter span.gsc_ecd_type_chapter,.gsc_ecd_form_book span.gsc_ecd_type_book,.gsc_ecd_form_thesis span.gsc_ecd_type_thesis,.gsc_ecd_form_patent span.gsc_ecd_type_patent,.gsc_ecd_form_courtcase span.gsc_ecd_type_courtcase,.gsc_ecd_form_other span.gsc_ecd_type_other{display:inline;}#gsc_ocd_view,#gsc_ocd_upload,.gsc_ocd_bdy_upload #gsc_ocd_edit,.gsc_ocd_bdy_view #gsc_ocd_edit,#gsc_ocd_error:empty{display:none;}.gsc_ocd_bdy_upload #gsc_ocd_upload,.gsc_ocd_bdy_view #gsc_ocd_view,#gsc_ocd_edit{display:block;}#gsc_ocd_error{padding:8px;margin-bottom:16px;}</style><div id="gsc_ocd_bdy" data-btns="" class="gsc_ocd_bdy_view"><div id="gsc_ocd_error" class="gs_alrt"></div><div id="gsc_ocd_view"><form method="post" action="/citations?view_op=edit_citation&amp;update_op=&amp;hl=en&amp;oe=ASCII" id="gsc_vcd_form" data-url="/citations?hl=en&amp;oe=ASCII"><input id="gsc_vcd_xsrf" type="hidden" name="xsrf" value="AMD79ooAAAAAX6jsv5K178gJsYy7cUpqlQlkMDyS8aLk"><input id="gsc_vcd_cid" type="hidden" name="s" value="0Eome9MAAAAJ:TFP_iSt0sucC"><div id="gsc_vcd_title_wrapper"><div id="gsc_vcd_title_gg"><div class="gsc_vcd_title_ggi"><a href="https://arxiv.org/pdf/1909.08970" data-clk="hl=en&amp;sa=T&amp;ei=P5unX9mnEcLPmAHXtJSIAQ&amp;scisig=AAGBfm017rltR57aM70rvpxSXfXDla7NSg&amp;nossl=1"><span class='gsc_vcd_title_ggt'>[PDF]</span> from arxiv.org</a></div></div><div id="gsc_vcd_title"><a class="gsc_vcd_title_link" href="https://arxiv.org/abs/1909.08970" data-clk="hl=en&amp;sa=T&amp;ei=P5unX9mnEcLPmAHXtJSIAQ&amp;scisig=AAGBfm0yWGv7oQY4a_XA6Z3WCmyrHlR6Cg&amp;nossl=1">RUN through the Streets: A New Dataset and Baseline Models for Realistic Urban Navigation</a></div></div><div id="gsc_vcd_table"><div class="gs_scl"><div class="gsc_vcd_field">Authors</div><div class="gsc_vcd_value">Tzuf Paz-Argaman, Reut Tsarfaty</div></div><div class="gs_scl"><div class="gsc_vcd_field">Publication date</div><div class="gsc_vcd_value">2019/9/19</div></div><div class="gs_scl"><div class="gsc_vcd_field">Journal</div><div class="gsc_vcd_value">arXiv preprint arXiv:1909.08970</div></div><div class="gs_scl"><div class="gsc_vcd_field">Description</div><div class="gsc_vcd_value" id="gsc_vcd_descr"><div class="gsh_small"><div class="gsh_csp">Following navigation instructions in natural language requires a composition of language, action, and knowledge of the environment. Knowledge of the environment may be provided via visual sensors or as a symbolic world representation referred to as a map. Here we introduce the Realistic Urban Navigation (RUN) task, aimed at interpreting navigation instructions based on a real, dense, urban map. Using Amazon Mechanical Turk, we collected a dataset of 2515 instructions aligned with actual routes over three regions of Manhattan. We propose a strong baseline for the task and empirically investigate which aspects of the neural architecture are important for the RUN success. Our results empirically show that entity abstraction, attention over words and worlds, and a constantly updating world-state, significantly contribute to task accuracy.</div></div></div></div><div class="gs_scl"><div class="gsc_vcd_field">Scholar articles</div><div class="gsc_vcd_value"><div class="gsc_vcd_merged_snippet"><div><a href="http://scholar.google.co.il/scholar?oi=bibs&amp;cluster=11909607783509088514&amp;btnI=1&amp;nossl=1&amp;hl=en&amp;oe=ASCII">RUN through the Streets: A New Dataset and Baseline Models for Realistic Urban Navigation</a></div><div>T Paz-Argaman, R Tsarfaty - arXiv preprint arXiv:1909.08970, 2019</div><div><a class="gsc_oms_link" href="https://scholar.google.co.il/scholar?oi=bibs&amp;hl=en&amp;oe=ASCII&amp;q=related:AllJ5i1tR6UJ:scholar.google.com/">Related articles</a> <a class="gsc_oms_link" href="https://scholar.google.co.il/scholar?oi=bibs&amp;hl=en&amp;oe=ASCII&amp;cluster=11909607783509088514">All 3 versions</a> </div></div></div></div></div></form></div><div id="gsc_ocd_edit"></div><div id="gsc_ocd_upload"></div></div>