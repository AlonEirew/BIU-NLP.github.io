<style>.gsc_oms_mm{font-size:24px;line-height:16px;display:inline-block;margin:-10px 0 0 4px;position:relative;top:8px;}.gsc_oms_link{white-space:nowrap;margin-right:12px;}.gsc_oms_link:last-child{margin-right:0;}#gsc_ocd_upload{max-width:500px;margin:0 auto;}.gsc_upl_title{font-size:20px;margin:0 0 4px 0;}.gsc_upl_desc{font-size:16px;padding:24px 0;}.gs_el_ph .gsc_upl_desc{padding:16px 0;}.gsc_upl_cprt{font-size:16px;color:#777;}#gsc_upl_error:empty,#gsc_upl_form{display:none;}#gsc_upl_error{padding:8px;margin-bottom:16px;}#gsc_vcd_title_wrapper{font-size:16px;margin:0 0 16px 0;}#gsc_vcd_title{font-size:20px;}.gs_el_sm #gsc_vcd_title{font-size:18px;}#gsc_vcd_title_gg{float:right;padding:0 0 8px 16px;}.gs_el_ph #gsc_vcd_title_gg{float:none;text-align:right;padding:0;margin:-4px 0 8px 0;}.gsc_vcd_title_ggt{font-size:13px;font-weight:bold;}.gsc_vcd_title_ggut{font-size:13px;color:#777;padding-bottom:8px;}.gsc_vcd_field{float:left;width:100px;text-align:right;color:#777;}.gs_el_ph .gsc_vcd_field{float:none;width:auto;text-align:left;margin-bottom:4px;font-size:16px;color:black;}.gsc_vcd_value{margin-left:116px;max-width:512px;margin-bottom:16px;}.gs_el_ph .gsc_vcd_value{margin-left:0;}.gsc_vcd_merged_snippet{margin-bottom:1em;}#gsc_vcd_graph{position:relative;height:100px;}#gsc_vcd_graph_x{position:absolute;top:57px;left:0;width:100%;height:13px;border-top:1px solid #777;}.gsc_vcd_g_t{position:absolute;top:60px;color:#777;font-size:11px;}#gsc_vcd_graph_bars{position:absolute;top:0;right:0;width:100%;height:100px;overflow-x:auto;}.gsc_vcd_g_a{position:absolute;bottom:13px;width:15px;background:#777;}.gsc_vcd_g_a:hover{background:#1a0dab;text-decoration:none;}.gsc_vcd_g_a:active{background:#d14836;}.gsc_vcd_g_al{position:absolute;bottom:15px;left:7px;color:#222;background:white;font-size:11px;min-width:11px;text-align:center;padding:1px;border:1px solid #777;border-radius:1px;visibility:hidden;opacity:0;}.gsc_vcd_g_a:hover .gsc_vcd_g_al{visibility:visible;opacity:1;}.gsc_vcd_g_a{transition:all .218s;}.gsc_vcd_g_al{transition:opacity .218s,visibility 0s .218s;}.gsc_vcd_g_a:hover,.gsc_vcd_g_a:hover .gsc_vcd_g_al{transition:all 0s;}a.gsc_vcd_lbx:link,a.gsc_vcd_lbx:visited{white-space:nowrap;display:inline-block;vertical-align:middle;font-size:11px;background-color:#e5e5e5;color:#777;padding:2px 6px;text-decoration:none;}a.gsc_vcd_lbx:hover{color:#e5e5e5;background-color:#777;}a.gsc_vcd_lbx:active{color:#d14836;}#gsc_ecd_alrt{padding:8px 16px;margin-bottom:12px;}#gsc_ecd_alrt:empty{display:none;}.gsc_ecd_field,.gsc_ecd_value{padding:6px 0;vertical-align:top;}.gsc_ecd_field{float:left;width:100px;text-align:right;color:#777;line-height:16px;padding:13px 0 6px 0;}.gs_el_ph .gsc_ecd_field,.gs_el_tc .gsc_ecd_field{float:none;width:auto;text-align:left;padding:8px 0 0 0;font-size:16px;color:black;}#gsc_ecd_citation_type{margin-bottom:12px;}.gsc_ecd_value,#gsc_ecd_citation_type{margin-left:116px;}.gs_el_ph .gsc_ecd_value,.gs_el_tc .gsc_ecd_value,.gs_el_ta #gsc_ecd_citation_type,.gs_el_ph #gsc_ecd_citation_type{margin-left:0;padding:8px 0;}.gs_el_ta #gsc_ecd_citation_type,.gs_el_ph #gsc_ecd_citation_type{text-align:center;}.gs_el_ph #gsc_ecd_citation_type{padding:0;margin-bottom:8px;}.gs_el_ph #gsc_ecd_title_value .gs_gray,.gs_el_tc #gsc_ecd_title_value .gs_gray,#gsc_ecd_reporter_value .gs_gray{display:none;}.gsc_ecd_reporter_line{margin-bottom:4px;}.gsc_ecd_merged_snippet{line-height:1.24;}.gsc_ecd_merged_radio{line-height:18px;margin:8px 0 16px 0;color:#555;}.gsc_ecd_type_journal,.gsc_ecd_type_conference,.gsc_ecd_type_chapter,.gsc_ecd_type_book,.gsc_ecd_type_thesis,.gsc_ecd_type_patent,.gsc_ecd_type_courtcase,.gsc_ecd_type_other{display:none;}.gsc_ecd_form_journal .gsc_ecd_type_journal,.gsc_ecd_form_conference .gsc_ecd_type_conference,.gsc_ecd_form_chapter .gsc_ecd_type_chapter,.gsc_ecd_form_book .gsc_ecd_type_book,.gsc_ecd_form_thesis .gsc_ecd_type_thesis,.gsc_ecd_form_patent .gsc_ecd_type_patent,.gsc_ecd_form_courtcase .gsc_ecd_type_courtcase,.gsc_ecd_form_other .gsc_ecd_type_other{display:block;}.gsc_ecd_form_journal span.gsc_ecd_type_journal,.gsc_ecd_form_conference span.gsc_ecd_type_conference,.gsc_ecd_form_chapter span.gsc_ecd_type_chapter,.gsc_ecd_form_book span.gsc_ecd_type_book,.gsc_ecd_form_thesis span.gsc_ecd_type_thesis,.gsc_ecd_form_patent span.gsc_ecd_type_patent,.gsc_ecd_form_courtcase span.gsc_ecd_type_courtcase,.gsc_ecd_form_other span.gsc_ecd_type_other{display:inline;}#gsc_ocd_view,#gsc_ocd_upload,.gsc_ocd_bdy_upload #gsc_ocd_edit,.gsc_ocd_bdy_view #gsc_ocd_edit,#gsc_ocd_error:empty{display:none;}.gsc_ocd_bdy_upload #gsc_ocd_upload,.gsc_ocd_bdy_view #gsc_ocd_view,#gsc_ocd_edit{display:block;}#gsc_ocd_error{padding:8px;margin-bottom:16px;}</style><div id="gsc_ocd_bdy" data-btns="" class="gsc_ocd_bdy_view"><div id="gsc_ocd_error" class="gs_alrt"></div><div id="gsc_ocd_view"><form method="post" action="/citations?view_op=edit_citation&amp;update_op=&amp;hl=en&amp;oe=ASCII" id="gsc_vcd_form" data-url="/citations?hl=en&amp;oe=ASCII"><input id="gsc_vcd_xsrf" type="hidden" name="xsrf" value="AMD79ooAAAAAX6poDWe5iOfMaBRdx-oeZY95nnXKc7xf"><input id="gsc_vcd_cid" type="hidden" name="s" value="0rskDKgAAAAJ:q1zXlPLtbUIC"><div id="gsc_vcd_title_wrapper"><div id="gsc_vcd_title_gg"><div class="gsc_vcd_title_ggi"><a href="https://arxiv.org/pdf/1912.13283" data-clk="hl=en&amp;sa=T&amp;ei=jRapX4G1BsLPmAHXtJSIAQ&amp;scisig=AAGBfm2_IVxPvQNlk8ic7dT9qZjjVKodVg&amp;nossl=1"><span class='gsc_vcd_title_ggt'>[PDF]</span> from arxiv.org</a></div></div><div id="gsc_vcd_title"><a class="gsc_vcd_title_link" href="https://arxiv.org/abs/1912.13283" data-clk="hl=en&amp;sa=T&amp;ei=jRapX4G1BsLPmAHXtJSIAQ&amp;scisig=AAGBfm2QBaURH58Zmm5XoQFARfVMtf-w-w&amp;nossl=1">oLMpics--On what Language Model Pre-training Captures</a></div></div><div id="gsc_vcd_table"><div class="gs_scl"><div class="gsc_vcd_field">Authors</div><div class="gsc_vcd_value">Alon Talmor, Yanai Elazar, Yoav Goldberg, Jonathan Berant</div></div><div class="gs_scl"><div class="gsc_vcd_field">Publication date</div><div class="gsc_vcd_value">2019/12/31</div></div><div class="gs_scl"><div class="gsc_vcd_field">Journal</div><div class="gsc_vcd_value">arXiv preprint arXiv:1912.13283</div></div><div class="gs_scl"><div class="gsc_vcd_field">Description</div><div class="gsc_vcd_value" id="gsc_vcd_descr"><div class="gsh_small"><div class="gsh_csp">Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition. A fundamental challenge is to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. To address this, we propose an evaluation protocol that includes both zero-shot evaluation (no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to the learning curve of multiple controls, which paints a rich picture of the LM capabilities. Our main findings are that:(a) different LMs exhibit qualitatively different reasoning abilities, eg, RoBERTa succeeds in reasoning tasks where BERT fails completely;(b) LMs do not reason in an abstract manner and are context-dependent, eg, while RoBERTa can compare ages, it can do so only when the ages are in the typical range of human ages;(c) On half of our reasoning tasks all models fail completely. Our findings and infrastructure can help future work on designing new datasets, models and objective functions for pre-training.</div></div></div></div><div class="gs_scl"><div class="gsc_vcd_field">Total citations</div><div class="gsc_vcd_value"><div style="margin-bottom:1em"><a href="https://scholar.google.co.il/scholar?oi=bibs&amp;hl=en&amp;oe=ASCII&amp;cites=13976401076540886840&amp;as_sdt=5">Cited by 32</a></div><div id="gsc_vcd_graph"><div id="gsc_vcd_graph_x"></div><div id="gsc_vcd_graph_bars"><span class="gsc_vcd_g_t" style="left:0px">2020</span><a href="https://scholar.google.co.il/scholar?oi=bibs&amp;hl=en&amp;oe=ASCII&amp;cites=13976401076540886840&amp;as_sdt=5&amp;as_ylo=2020&amp;as_yhi=2020" class="gsc_vcd_g_a" style="left:5px;height:57px;top:0px;z-index:1"><span class="gsc_vcd_g_al">32</span></a></div></div></div></div><div class="gs_scl"><div class="gsc_vcd_field">Scholar articles</div><div class="gsc_vcd_value"><div class="gsc_vcd_merged_snippet"><div><a href="http://scholar.google.co.il/scholar?oi=bibs&amp;cluster=13976401076540886840&amp;btnI=1&amp;nossl=1&amp;hl=en&amp;oe=ASCII">oLMpics--On what Language Model Pre-training Captures</a></div><div>A Talmor, Y Elazar, Y Goldberg, J Berant - arXiv preprint arXiv:1912.13283, 2019</div><div><a class="gsc_oms_link" href="https://scholar.google.co.il/scholar?oi=bibs&amp;hl=en&amp;oe=ASCII&amp;cites=13976401076540886840&amp;as_sdt=5">Cited by 32</a> <a class="gsc_oms_link" href="https://scholar.google.co.il/scholar?oi=bibs&amp;hl=en&amp;oe=ASCII&amp;q=related:OE-bnbom9sEJ:scholar.google.com/">Related articles</a> <a class="gsc_oms_link" href="https://scholar.google.co.il/scholar?oi=bibs&amp;hl=en&amp;oe=ASCII&amp;cluster=13976401076540886840">All 2 versions</a> </div></div></div></div></div></form></div><div id="gsc_ocd_edit"></div><div id="gsc_ocd_upload"></div></div>